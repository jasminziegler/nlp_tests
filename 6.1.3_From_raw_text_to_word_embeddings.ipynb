{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designed-patrick",
   "metadata": {},
   "source": [
    "## From raw text to word embeddings using pretrained embeddings from 2014 English Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-updating",
   "metadata": {},
   "source": [
    "Downloading the IMDB data as raw text, load reviews info list of strings - one string per review, prepare labels - neg=0, pos=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saved-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/development/datasets/aclImdb/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = '/home/user/development/datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "horizontal-vessel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1.3_From_raw_text_to_word_embeddings.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "painful-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'cd': No such file or directory\n",
      "../:\n",
      "base_image\t datasets      docker-JabRef  Pdatascience  test\n",
      "build_image_gpu  docker-ionic  nlp_tests      README.md\n"
     ]
    }
   ],
   "source": [
    "! ls cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "married-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []  # read in all txt files from neg and pos\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-uganda",
   "metadata": {},
   "source": [
    "Tokenize the data - vectorize text, prepare training and validation split - we want to simulate having \"little training data\" to show how good pretrained word embeddings work so: we restrict training data to the first 200 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "present-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42557 unique tokens.\n",
      "Shape of data tensor: (5674, 100)\n",
      "Shape of label tensor: (5674,)\n",
      "Shape x_train:  (200, 100)\n",
      "Shape y_train:  (200,)\n",
      "Shape x_val:  (5474, 100)\n",
      "Shape y_val:  (5474,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # cut off reviews after 100 words\n",
    "training_samples = 200  # only train on 200 samples to mimic having little training data\n",
    "validation_samples = 10000  # validate on 10k samples\n",
    "max_words = 10000  # consider only top 10k words in dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)  # turns lists of sequences to 2D tensors of shape (sequences, maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:',labels.shape)\n",
    "\n",
    "#splits data into training set and validation set, but first shuffles the data bec we read in all neg and then all pos data so they are ordered\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples+validation_samples]\n",
    "y_val = labels[training_samples: training_samples+validation_samples]\n",
    "\n",
    "print('Shape x_train: ', x_train.shape)\n",
    "print('Shape y_train: ', y_train.shape)\n",
    "print('Shape x_val: ', x_val.shape)\n",
    "print('Shape y_val: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-findings",
   "metadata": {},
   "source": [
    "Downloading the GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "experienced-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME=\"Ubuntu\"\n",
      "VERSION=\"20.04.1 LTS (Focal Fossa)\"\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "PRETTY_NAME=\"Ubuntu 20.04.1 LTS\"\n",
      "VERSION_ID=\"20.04\"\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "VERSION_CODENAME=focal\n",
      "UBUNTU_CODENAME=focal\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: effective uid is not 0, is /usr/bin/sudo on a file system with the 'nosuid' option set or an NFS file system without root privileges?\n"
     ]
    }
   ],
   "source": [
    "! sudo apt install curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: curl: command not found\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = 'glove'\n",
    "\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7881bea",
   "metadata": {},
   "source": [
    "# move to version 2 of the book Deep Learning with Python, F. Chollet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bc1db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
